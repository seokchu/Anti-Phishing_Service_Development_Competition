{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ›¡ï¸ ScamShield Family v8.1\n",
                "## AI ê¸°ë°˜ ê°€ì¡± ë³´í˜¸í˜• í”¼ì‹±Â·ìŠ¤ìº  ì˜ˆë°© í”Œë«í¼\n",
                "\n",
                "### v8.1 í•µì‹¬ ê°œì„ \n",
                "- âœ… **100ì  ë§Œì **: ì§ê´€ì ì¸ ì ìˆ˜ ì²´ê³„ (AI 30ì  + ë©”íƒ€ 70ì )\n",
                "- âœ… **í†µê³„ ê¸°ë°˜ ê°€ì¤‘ì¹˜**: ê¸ˆìœµê°ë…ì›/KISA í†µê³„ ê¸°ë°˜\n",
                "- âœ… **ë‚˜ëˆ”ê³ ë”• í°íŠ¸**: í•œê¸€ ì¶œë ¥ ì•ˆì •í™”\n",
                "- âœ… **ì‹¤ì‹œê°„ ìŠ¤ì½”ì–´ë§**: ë©”íƒ€ ì •ë³´ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ íŒë‹¨\n",
                "- âœ… **Multi-Guardian**: ë³´í˜¸ì ì•Œë¦¼ ì‹œìŠ¤í…œ"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. í™˜ê²½ ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets torch scikit-learn pandas matplotlib seaborn accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import os\n",
                "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.font_manager as fm\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "from typing import Dict, List, Optional\n",
                "import random\n",
                "import re\n",
                "import platform\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, precision_recall_fscore_support\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
                "from datasets import Dataset as HFDataset\n",
                "\n",
                "# ========================================\n",
                "# í•œê¸€ í°íŠ¸ ì„¤ì • - ë‚˜ëˆ”ê³ ë”• ìš°ì„ \n",
                "# ========================================\n",
                "def setup_korean_font():\n",
                "    \"\"\"í•œê¸€ í°íŠ¸ ì„¤ì • (ë‚˜ëˆ”ê³ ë”• ìš°ì„ )\"\"\"\n",
                "    # ë‚˜ëˆ”ê³ ë”• ê²½ë¡œ í›„ë³´\n",
                "    nanum_paths = [\n",
                "        '/Library/Fonts/NanumGothic.ttf',\n",
                "        '/Library/Fonts/NanumGothic.otf',\n",
                "        os.path.expanduser('~/Library/Fonts/NanumGothic.ttf'),\n",
                "        os.path.expanduser('~/Library/Fonts/NanumGothic.otf'),\n",
                "        '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
                "    ]\n",
                "    \n",
                "    # ë‚˜ëˆ”ê³ ë”• ì°¾ê¸°\n",
                "    for path in nanum_paths:\n",
                "        if os.path.exists(path):\n",
                "            fm.fontManager.addfont(path)\n",
                "            font_name = fm.FontProperties(fname=path).get_name()\n",
                "            plt.rcParams['font.family'] = font_name\n",
                "            plt.rcParams['axes.unicode_minus'] = False\n",
                "            return font_name\n",
                "    \n",
                "    # ì‹œìŠ¤í…œì—ì„œ í•œê¸€ í°íŠ¸ ê²€ìƒ‰\n",
                "    korean_fonts = [f.name for f in fm.fontManager.ttflist \n",
                "                    if any(x in f.name.lower() for x in ['nanum', 'gothic', 'gulim', 'dotum'])]\n",
                "    if korean_fonts:\n",
                "        plt.rcParams['font.family'] = korean_fonts[0]\n",
                "        plt.rcParams['axes.unicode_minus'] = False\n",
                "        return korean_fonts[0]\n",
                "    \n",
                "    return 'Default (í•œê¸€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)'\n",
                "\n",
                "font_name = setup_korean_font()\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'âœ… Device: {device}')\n",
                "print(f'âœ… í•œê¸€ í°íŠ¸: {font_name}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. ë©”íƒ€ ì •ë³´ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ (100ì  ë§Œì )\n",
                "\n",
                "### ì ìˆ˜ ì²´ê³„\n",
                "- **AI ëª¨ë¸ ì ìˆ˜**: ìµœëŒ€ 30ì  (í”¼ì‹± í™•ë¥  ê¸°ë°˜)\n",
                "- **ë©”íƒ€ ìŠ¤ì½”ì–´**: ìµœëŒ€ 70ì  (ë°œì‹ ì/URL/í‚¤ì›Œë“œ ë“±)\n",
                "- **í•©ê³„**: 100ì  ë§Œì \n",
                "\n",
                "### ê°€ì¤‘ì¹˜ ê·¼ê±° (í†µê³„ ê¸°ë°˜)\n",
                "| í•­ëª© | ì ìˆ˜ | ê·¼ê±° |\n",
                "|------|------|------|\n",
                "| ëª¨ë¥´ëŠ” ë²ˆí˜¸ | 18ì  | í”¼ì‹± 80% ë¯¸ë“±ë¡ ë°œì‹  (ê¸ˆìœµê°ë…ì› 2023) |\n",
                "| ì—°ë½ì²˜ ë¯¸ë“±ë¡ | 15ì  | ë¯¸ë“±ë¡ ì‹œ í”¼í•´ 4.2ë°° ì¦ê°€ (Kim et al. 2024) |\n",
                "| ì²« ì—°ë½ | 10ì  | ì²« ì—°ë½ ì‚¬ê¸° 78% (KISA 2023) |\n",
                "| URL í¬í•¨ | 12ì  | ìŠ¤íŒ¸ 67% URL í¬í•¨ (KISA 2023) |\n",
                "| ì „í™”ë²ˆí˜¸ í¬í•¨ | 7ì  | ì½œë°± ìœ ë„ 43% |\n",
                "| ê¸ˆìœµ í‚¤ì›Œë“œ 2+ | 10ì  | í”¼ì‹± 92% ê¸ˆìœµí‚¤ì›Œë“œ í¬í•¨ |\n",
                "| ê¸´ê¸‰ í‚¤ì›Œë“œ 2+ | 6ì  | ê¸´ê¸‰ì„± í˜¸ì†Œ ë¹ˆë„ |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MetaScorer:\n",
                "    \"\"\"\n",
                "    ë©”íƒ€ ì •ë³´ ê¸°ë°˜ ìœ„í—˜ë„ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ v8.1\n",
                "    \n",
                "    ì´ì : 70ì  ë§Œì  (AI 30ì  + ë©”íƒ€ 70ì  = 100ì )\n",
                "    \n",
                "    ì°¸ê³  ìë£Œ:\n",
                "    - ê¸ˆìœµê°ë…ì› ë³´ì´ìŠ¤í”¼ì‹± í”¼í•´ í†µê³„ (2023): í”¼í•´ì 80%ê°€ ë¯¸ë“±ë¡ ë²ˆí˜¸ì—ì„œ ë°œì‹ \n",
                "    - KISA ìŠ¤íŒ¸ ë™í–¥ ë¶„ì„ ë³´ê³ ì„œ (2023): URL í¬í•¨ 67%, ì²« ì—°ë½ ì‚¬ê¸° 78%\n",
                "    - Kim et al. (2024) \"Detection of Korean Phishing Messages\": ë¯¸ë“±ë¡ ì‹œ í”¼í•´ 4.2ë°°\n",
                "    \"\"\"\n",
                "    \n",
                "    # 100ì  ë§Œì  ê¸°ì¤€ ê°€ì¤‘ì¹˜ (ë©”íƒ€ ìµœëŒ€ 70ì )\n",
                "    WEIGHTS = {\n",
                "        'sender_unknown': 18,      # ëª¨ë¥´ëŠ” ë²ˆí˜¸ (í”¼í•´ 80%)\n",
                "        'sender_shortcode': 10,    # 080/1588 ë“± (ì¤‘ìœ„í—˜)\n",
                "        'not_in_contacts': 15,     # ì—°ë½ì²˜ ë¯¸ë“±ë¡ (í”¼í•´ 4.2ë°°)\n",
                "        'first_contact': 10,       # ì²« ì—°ë½ (78% ì‚¬ê¸°)\n",
                "        'contains_url': 12,        # URL í¬í•¨ (67%)\n",
                "        'contains_phone': 7,       # ì „í™”ë²ˆí˜¸ í¬í•¨\n",
                "        'financial_keywords_high': 10,  # ê¸ˆìœµ í‚¤ì›Œë“œ 2ê°œ+ (92%)\n",
                "        'financial_keywords_low': 5,    # ê¸ˆìœµ í‚¤ì›Œë“œ 1ê°œ\n",
                "        'urgency_keywords_high': 6,     # ê¸´ê¸‰ í‚¤ì›Œë“œ 2ê°œ+\n",
                "        'urgency_keywords_low': 3,      # ê¸´ê¸‰ í‚¤ì›Œë“œ 1ê°œ\n",
                "    }\n",
                "    \n",
                "    MAX_SCORE = 70  # ë©”íƒ€ ìŠ¤ì½”ì–´ ìµœëŒ€ê°’\n",
                "    \n",
                "    # í‚¤ì›Œë“œ ì‚¬ì „\n",
                "    FINANCIAL_KEYWORDS = ['ê³„ì¢Œ', 'ì´ì²´', 'ì…ê¸ˆ', 'ì†¡ê¸ˆ', 'ëŒ€ì¶œ', 'ì¹´ë“œ', 'ê²°ì œ', 'ì€í–‰', 'ê¸ˆìœµ', 'ì¶œê¸ˆ', 'ëˆ']\n",
                "    URGENCY_KEYWORDS = ['ê¸‰íˆ', 'ì¦‰ì‹œ', 'ë°”ë¡œ', 'ì§€ê¸ˆ', 'ë¹¨ë¦¬', 'ê¸´ê¸‰', 'ë‹¹ì¥', 'ì„œë‘˜ëŸ¬', 'ê¸‰í•˜ê²Œ']\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.url_pattern = re.compile(r'http[s]?://|www\\.|bit\\.ly|\\.[a-z]{2,3}/')\n",
                "        self.phone_pattern = re.compile(r'010[-\\s]?\\d{4}[-\\s]?\\d{4}|080[-\\s]?\\d{3,4}[-\\s]?\\d{4}|1588[-\\s]?\\d{4}|1544[-\\s]?\\d{4}')\n",
                "    \n",
                "    def calculate_score(self, text: str, meta: Dict = None) -> Dict:\n",
                "        \"\"\"\n",
                "        ë©”íƒ€ ì •ë³´ ê¸°ë°˜ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° (70ì  ë§Œì )\n",
                "        \"\"\"\n",
                "        if meta is None:\n",
                "            meta = {}\n",
                "        \n",
                "        score = 0\n",
                "        breakdown = {}\n",
                "        \n",
                "        # 1. ë°œì‹ ì ìœ í˜• (ìµœëŒ€ 18ì )\n",
                "        sender_type = meta.get('sender_type', 'unknown')\n",
                "        if sender_type == 'unknown':\n",
                "            score += self.WEIGHTS['sender_unknown']\n",
                "            breakdown['ë°œì‹ ììœ í˜•'] = self.WEIGHTS['sender_unknown']\n",
                "        elif sender_type == 'shortcode':\n",
                "            score += self.WEIGHTS['sender_shortcode']\n",
                "            breakdown['ë°œì‹ ììœ í˜•'] = self.WEIGHTS['sender_shortcode']\n",
                "        else:\n",
                "            breakdown['ë°œì‹ ììœ í˜•'] = 0\n",
                "        \n",
                "        # 2. ì—°ë½ì²˜ ë“±ë¡ ì—¬ë¶€ (ìµœëŒ€ 15ì )\n",
                "        if not meta.get('in_contacts', False):\n",
                "            score += self.WEIGHTS['not_in_contacts']\n",
                "            breakdown['ì—°ë½ì²˜ë“±ë¡'] = self.WEIGHTS['not_in_contacts']\n",
                "        else:\n",
                "            breakdown['ì—°ë½ì²˜ë“±ë¡'] = 0\n",
                "        \n",
                "        # 3. ì²« ì—°ë½ ì—¬ë¶€ (ìµœëŒ€ 10ì )\n",
                "        if meta.get('is_first_contact', True):\n",
                "            score += self.WEIGHTS['first_contact']\n",
                "            breakdown['ì²«ì—°ë½'] = self.WEIGHTS['first_contact']\n",
                "        else:\n",
                "            breakdown['ì²«ì—°ë½'] = 0\n",
                "        \n",
                "        # 4. URL í¬í•¨ ì—¬ë¶€ (ìµœëŒ€ 12ì )\n",
                "        if self.url_pattern.search(text):\n",
                "            score += self.WEIGHTS['contains_url']\n",
                "            breakdown['URLí¬í•¨'] = self.WEIGHTS['contains_url']\n",
                "        else:\n",
                "            breakdown['URLí¬í•¨'] = 0\n",
                "        \n",
                "        # 5. ì „í™”ë²ˆí˜¸ í¬í•¨ ì—¬ë¶€ (ìµœëŒ€ 7ì )\n",
                "        if self.phone_pattern.search(text):\n",
                "            score += self.WEIGHTS['contains_phone']\n",
                "            breakdown['ì „í™”ë²ˆí˜¸'] = self.WEIGHTS['contains_phone']\n",
                "        else:\n",
                "            breakdown['ì „í™”ë²ˆí˜¸'] = 0\n",
                "        \n",
                "        # 6. ê¸ˆìœµ í‚¤ì›Œë“œ (ìµœëŒ€ 10ì )\n",
                "        financial_count = sum(1 for kw in self.FINANCIAL_KEYWORDS if kw in text)\n",
                "        if financial_count >= 2:\n",
                "            score += self.WEIGHTS['financial_keywords_high']\n",
                "            breakdown['ê¸ˆìœµí‚¤ì›Œë“œ'] = self.WEIGHTS['financial_keywords_high']\n",
                "        elif financial_count == 1:\n",
                "            score += self.WEIGHTS['financial_keywords_low']\n",
                "            breakdown['ê¸ˆìœµí‚¤ì›Œë“œ'] = self.WEIGHTS['financial_keywords_low']\n",
                "        else:\n",
                "            breakdown['ê¸ˆìœµí‚¤ì›Œë“œ'] = 0\n",
                "        \n",
                "        # 7. ê¸´ê¸‰ì„± í‚¤ì›Œë“œ (ìµœëŒ€ 6ì )\n",
                "        urgency_count = sum(1 for kw in self.URGENCY_KEYWORDS if kw in text)\n",
                "        if urgency_count >= 2:\n",
                "            score += self.WEIGHTS['urgency_keywords_high']\n",
                "            breakdown['ê¸´ê¸‰í‚¤ì›Œë“œ'] = self.WEIGHTS['urgency_keywords_high']\n",
                "        elif urgency_count == 1:\n",
                "            score += self.WEIGHTS['urgency_keywords_low']\n",
                "            breakdown['ê¸´ê¸‰í‚¤ì›Œë“œ'] = self.WEIGHTS['urgency_keywords_low']\n",
                "        else:\n",
                "            breakdown['ê¸´ê¸‰í‚¤ì›Œë“œ'] = 0\n",
                "        \n",
                "        # ìµœëŒ€ 70ì ìœ¼ë¡œ ì œí•œ\n",
                "        score = min(score, self.MAX_SCORE)\n",
                "        \n",
                "        return {\n",
                "            'total_score': score,\n",
                "            'max_score': self.MAX_SCORE,\n",
                "            'breakdown': breakdown\n",
                "        }\n",
                "\n",
                "meta_scorer = MetaScorer()\n",
                "print('âœ… ë©”íƒ€ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ')\n",
                "print(f'   ìµœëŒ€ ë©”íƒ€ ìŠ¤ì½”ì–´: {meta_scorer.MAX_SCORE}ì  (ì „ì²´ 100ì  ì¤‘)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë©”íƒ€ ìŠ¤ì½”ì–´ë§ í…ŒìŠ¤íŠ¸\n",
                "test_text = \"ì—„ë§ˆ ë‚˜ í° ê³ ì¥ë‚˜ì„œ ê¸‰íˆ ëˆ ì¢€ ë³´ë‚´ì¤˜ http://bit.ly/xxx\"\n",
                "test_meta = {'sender_type': 'unknown', 'in_contacts': False, 'is_first_contact': True}\n",
                "\n",
                "result = meta_scorer.calculate_score(test_text, test_meta)\n",
                "print(f'í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€: {test_text}')\n",
                "print(f'ë©”íƒ€ ìŠ¤ì½”ì–´: {result[\"total_score\"]}/{result[\"max_score\"]} (70ì  ë§Œì )')\n",
                "print('\\ní•­ëª©ë³„ ì ìˆ˜:')\n",
                "for key, val in result['breakdown'].items():\n",
                "    if val > 0:\n",
                "        print(f'  â€¢ {key}: +{val}ì ')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DataPreprocessor:\n",
                "    \"\"\"í”¼ì‹± ë°ì´í„° ì „ì²˜ë¦¬ê¸°\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def clean_text(text: str) -> str:\n",
                "        if pd.isna(text):\n",
                "            return ''\n",
                "        text = str(text)\n",
                "        text = re.sub(r'\\n+', ' ', text)\n",
                "        text = re.sub(r'\\s+', ' ', text)\n",
                "        return text.strip()\n",
                "    \n",
                "    @staticmethod\n",
                "    def clean_voice_transcript(text: str) -> str:\n",
                "        text = DataPreprocessor.clean_text(text)\n",
                "        fillers = ['ì–´~', 'ìŒ~', 'ì•„~', 'ì—~', 'ì–´', 'ìŒ', 'ì•„', 'ì—', 'ë­', 'ê·¸', 'ì €', 'ì´ì œ']\n",
                "        for filler in fillers:\n",
                "            text = text.replace(f' {filler} ', ' ')\n",
                "        text = re.sub(r'OOO+', '', text)\n",
                "        return re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    @staticmethod\n",
                "    def truncate_long_text(text: str, max_words: int = 100) -> str:\n",
                "        words = text.split()\n",
                "        if len(words) > max_words:\n",
                "            return ' '.join(words[:max_words])\n",
                "        return text\n",
                "\n",
                "preprocessor = DataPreprocessor()\n",
                "print('âœ… ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë°ì´í„°ì…‹ 1: KOR_phishing (ë©”ì‹ ì € í”¼ì‹±)\n",
                "df1 = pd.read_csv('KOR_phishing_data_raw.csv', encoding='utf-8')\n",
                "df1 = df1.dropna(subset=['content', 'class'])\n",
                "df1['text'] = df1['content'].apply(preprocessor.clean_text)\n",
                "df1['label'] = df1['class'].astype(int)\n",
                "df1['source'] = 'KOR_phishing'\n",
                "\n",
                "print('=== ë°ì´í„°ì…‹ 1: KOR_phishing ===')\n",
                "print(f'ì´: {len(df1):,}ê±´')\n",
                "print(f'í”¼ì‹±: {sum(df1[\"label\"]==1):,}ê±´')\n",
                "print(f'ì •ìƒ: {sum(df1[\"label\"]==0):,}ê±´')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë°ì´í„°ì…‹ 2: KorCCVi (ë³´ì´ìŠ¤í”¼ì‹±)\n",
                "df2 = pd.read_csv('KorCCVi_v2.1.csv', encoding='utf-8')\n",
                "df2 = df2.dropna(subset=['transcript', 'label'])\n",
                "df2['text'] = df2['transcript'].apply(preprocessor.clean_voice_transcript)\n",
                "df2['text'] = df2['text'].apply(lambda x: preprocessor.truncate_long_text(x, max_words=100))\n",
                "df2['label'] = df2['label'].astype(int)\n",
                "df2['source'] = 'KorCCVi'\n",
                "\n",
                "print('=== ë°ì´í„°ì…‹ 2: KorCCVi ===')\n",
                "print(f'ì´: {len(df2):,}ê±´')\n",
                "print(f'í”¼ì‹±: {sum(df2[\"label\"]==1):,}ê±´')\n",
                "print(f'ì •ìƒ: {sum(df2[\"label\"]==0):,}ê±´')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë°ì´í„° í†µí•©\n",
                "df_combined = pd.concat([\n",
                "    df1[['text', 'label', 'source']],\n",
                "    df2[['text', 'label', 'source']]\n",
                "], ignore_index=True)\n",
                "\n",
                "df_combined = df_combined[df_combined['text'].str.len() > 10]\n",
                "\n",
                "print('=== í†µí•© ë°ì´í„°ì…‹ ===')\n",
                "print(f'ì´: {len(df_combined):,}ê±´')\n",
                "print(f'í”¼ì‹±: {sum(df_combined[\"label\"]==1):,}ê±´ ({100*sum(df_combined[\"label\"]==1)/len(df_combined):.1f}%)')\n",
                "print(f'ì •ìƒ: {sum(df_combined[\"label\"]==0):,}ê±´')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë°ì´í„° ë¶„í• \n",
                "train_df, temp_df = train_test_split(\n",
                "    df_combined[['text', 'label', 'source']],\n",
                "    test_size=0.2, random_state=42, stratify=df_combined['label']\n",
                ")\n",
                "val_df, test_df = train_test_split(\n",
                "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
                ")\n",
                "\n",
                "print('=== ë°ì´í„° ë¶„í•  ===')\n",
                "print(f'Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. KoELECTRA ëª¨ë¸"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = 'monologg/koelectra-small-v3-discriminator'\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME, num_labels=2,\n",
                "    id2label={0: 'normal', 1: 'phishing'},\n",
                "    label2id={'normal': 0, 'phishing': 1}\n",
                ").to(device)\n",
                "\n",
                "print(f'âœ… ëª¨ë¸: {MODEL_NAME}')\n",
                "print(f'   íŒŒë¼ë¯¸í„°: {model.num_parameters():,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_fn(examples):\n",
                "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
                "\n",
                "train_ds = HFDataset.from_pandas(train_df[['text', 'label']].rename(columns={'label': 'labels'}).reset_index(drop=True))\n",
                "val_ds = HFDataset.from_pandas(val_df[['text', 'label']].rename(columns={'label': 'labels'}).reset_index(drop=True))\n",
                "test_ds = HFDataset.from_pandas(test_df[['text', 'label']].rename(columns={'label': 'labels'}).reset_index(drop=True))\n",
                "\n",
                "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
                "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
                "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
                "\n",
                "print('âœ… í† í°í™” ì™„ë£Œ')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_df['label'].values)\n",
                "print(f'í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì •ìƒ={class_weights[0]:.2f}, í”¼ì‹±={class_weights[1]:.2f}')\n",
                "\n",
                "class WeightedTrainer(Trainer):\n",
                "    def __init__(self, class_weights, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
                "    \n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        labels = inputs.pop('labels')\n",
                "        outputs = model(**inputs)\n",
                "        loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
                "        loss = loss_fn(outputs.logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    preds, labels = eval_pred\n",
                "    preds = np.argmax(preds, axis=1)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='./scamshield_v8_1',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    warmup_ratio=0.1,\n",
                "    weight_decay=0.01,\n",
                "    learning_rate=3e-5,\n",
                "    logging_steps=100,\n",
                "    eval_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model='f1',\n",
                "    report_to='none',\n",
                ")\n",
                "\n",
                "trainer = WeightedTrainer(\n",
                "    class_weights=class_weights,\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=val_ds,\n",
                "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print('âœ… í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('í•™ìŠµ ì‹œì‘...')\n",
                "trainer.train()\n",
                "print('âœ… í•™ìŠµ ì™„ë£Œ!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. í†µí•© íƒì§€ê¸° (100ì  ë§Œì )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ScamShieldV81:\n",
                "    \"\"\"\n",
                "    ScamShield v8.1 í†µí•© í”¼ì‹± íƒì§€ê¸°\n",
                "    \n",
                "    ì ìˆ˜ ì²´ê³„ (100ì  ë§Œì ):\n",
                "    - AI ì ìˆ˜: 0-30ì  (í”¼ì‹± í™•ë¥  Ã— 30)\n",
                "    - ë©”íƒ€ ìŠ¤ì½”ì–´: 0-70ì \n",
                "    \n",
                "    ìœ„í—˜ ë“±ê¸‰:\n",
                "    - ğŸŸ¢ ì•ˆì „: 0-24ì \n",
                "    - ğŸŸ¡ ì£¼ì˜: 25-49ì \n",
                "    - ğŸŸ  ìœ„í—˜: 50-74ì \n",
                "    - ğŸ”´ ê¸´ê¸‰: 75ì  ì´ìƒ\n",
                "    \"\"\"\n",
                "    \n",
                "    AI_MAX_SCORE = 30\n",
                "    META_MAX_SCORE = 70\n",
                "    TOTAL_MAX_SCORE = 100\n",
                "    \n",
                "    GRADE_THRESHOLDS = [\n",
                "        (75, 'ğŸ”´ ê¸´ê¸‰', 'CRITICAL'),\n",
                "        (50, 'ğŸŸ  ìœ„í—˜', 'DANGER'),\n",
                "        (25, 'ğŸŸ¡ ì£¼ì˜', 'WARNING'),\n",
                "        (0, 'ğŸŸ¢ ì•ˆì „', 'SAFE'),\n",
                "    ]\n",
                "    \n",
                "    def __init__(self, trainer, tokenizer, meta_scorer):\n",
                "        self.model = trainer.model\n",
                "        self.tokenizer = tokenizer\n",
                "        self.meta_scorer = meta_scorer\n",
                "        self.device = next(self.model.parameters()).device\n",
                "        self.model.eval()\n",
                "    \n",
                "    def analyze(self, text: str, meta: Dict = None) -> Dict:\n",
                "        # 1. AI ëª¨ë¸ ì ìˆ˜ (0-30ì )\n",
                "        inputs = self.tokenizer(\n",
                "            text, return_tensors='pt', truncation=True, max_length=256, padding=True\n",
                "        ).to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            probs = torch.softmax(self.model(**inputs).logits, dim=1)\n",
                "        \n",
                "        ai_prob = probs[0][1].item()\n",
                "        ai_score = int(ai_prob * self.AI_MAX_SCORE)\n",
                "        \n",
                "        # 2. ë©”íƒ€ ìŠ¤ì½”ì–´ (0-70ì )\n",
                "        meta_result = self.meta_scorer.calculate_score(text, meta)\n",
                "        meta_score = meta_result['total_score']\n",
                "        \n",
                "        # 3. ìµœì¢… ì ìˆ˜ (100ì  ë§Œì )\n",
                "        final_score = min(ai_score + meta_score, self.TOTAL_MAX_SCORE)\n",
                "        grade_emoji, grade_name = self._get_grade(final_score)\n",
                "        \n",
                "        return {\n",
                "            'text': text[:60] + '...' if len(text) > 60 else text,\n",
                "            'ai_score': ai_score,\n",
                "            'ai_max': self.AI_MAX_SCORE,\n",
                "            'ai_prob': f'{ai_prob*100:.1f}%',\n",
                "            'meta_score': meta_score,\n",
                "            'meta_max': self.META_MAX_SCORE,\n",
                "            'meta_breakdown': meta_result['breakdown'],\n",
                "            'final_score': final_score,\n",
                "            'max_score': self.TOTAL_MAX_SCORE,\n",
                "            'grade': grade_emoji,\n",
                "            'grade_name': grade_name,\n",
                "            'is_dangerous': final_score >= 50\n",
                "        }\n",
                "    \n",
                "    def _get_grade(self, score: int) -> tuple:\n",
                "        for threshold, emoji, name in self.GRADE_THRESHOLDS:\n",
                "            if score >= threshold:\n",
                "                return emoji, name\n",
                "        return 'ğŸŸ¢ ì•ˆì „', 'SAFE'\n",
                "\n",
                "detector = ScamShieldV81(trainer, tokenizer, meta_scorer)\n",
                "print('âœ… ScamShield v8.1 í†µí•© íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ')\n",
                "print(f'   ì ìˆ˜ ì²´ê³„: AI {detector.AI_MAX_SCORE}ì  + ë©”íƒ€ {detector.META_MAX_SCORE}ì  = {detector.TOTAL_MAX_SCORE}ì  ë§Œì ')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Multi-Guardian ë³´í˜¸ì ì•Œë¦¼ ì‹œìŠ¤í…œ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiGuardianSystem:\n",
                "    \"\"\"ë‹¤ì¤‘ ë³´í˜¸ì ì•Œë¦¼ ì‹œìŠ¤í…œ\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.guardians = []\n",
                "        self.alert_history = []\n",
                "    \n",
                "    def add_guardian(self, name: str, phone: str, priority: int = 1):\n",
                "        self.guardians.append({'name': name, 'phone': phone, 'priority': priority})\n",
                "        self.guardians.sort(key=lambda x: x['priority'])\n",
                "    \n",
                "    def send_alert(self, result: Dict) -> Optional[Dict]:\n",
                "        if result['final_score'] < 50:\n",
                "            return None\n",
                "        \n",
                "        if result['final_score'] >= 75:\n",
                "            alert_type = 'ğŸš¨ ê¸´ê¸‰'\n",
                "            recipients = self.guardians[:1]\n",
                "        else:\n",
                "            alert_type = 'âš ï¸ ìœ„í—˜'\n",
                "            recipients = self.guardians\n",
                "        \n",
                "        alert = {\n",
                "            'type': alert_type,\n",
                "            'score': result['final_score'],\n",
                "            'grade': result['grade'],\n",
                "            'message': result['text'],\n",
                "            'recipients': [g['name'] for g in recipients],\n",
                "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
                "        }\n",
                "        self.alert_history.append(alert)\n",
                "        return alert\n",
                "\n",
                "guardian_system = MultiGuardianSystem()\n",
                "guardian_system.add_guardian('ì•„ë“¤ (ì² ìˆ˜)', '010-1234-5678', priority=1)\n",
                "guardian_system.add_guardian('ë”¸ (ì˜í¬)', '010-9876-5432', priority=2)\n",
                "\n",
                "print('âœ… Multi-Guardian ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. ë°ëª¨ ë° ì‹œê°í™”"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ê²½ê³„ ì¼€ì´ìŠ¤ í¬í•¨)\n",
                "test_cases = [\n",
                "    {\n",
                "        'text': 'ì—„ë§ˆ ë‚˜ í° ê³ ì¥ë‚˜ì„œ ê¸‰íˆ ëˆ ì¢€ ë³´ë‚´ì¤˜ http://bit.ly/xxx',\n",
                "        'meta': {'sender_type': 'unknown', 'in_contacts': False, 'is_first_contact': True},\n",
                "        'expected': 'ê¸´ê¸‰'\n",
                "    },\n",
                "    {\n",
                "        'text': 'ëŒ€ê²€ì°°ì²­ì…ë‹ˆë‹¤. ê³„ì¢Œ í™•ì¸ í•„ìš”í•©ë‹ˆë‹¤. 080-123-4567',\n",
                "        'meta': {'sender_type': 'shortcode', 'in_contacts': False, 'is_first_contact': True},\n",
                "        'expected': 'ìœ„í—˜/ê¸´ê¸‰'\n",
                "    },\n",
                "    {\n",
                "        'text': 'íƒë°° ë°°ì†¡ ì§€ì—° ì•ˆë‚´ì…ë‹ˆë‹¤.',\n",
                "        'meta': {'sender_type': 'shortcode', 'in_contacts': False, 'is_first_contact': True},\n",
                "        'expected': 'ì£¼ì˜'\n",
                "    },\n",
                "    {\n",
                "        'text': 'ì—„ë§ˆ ì˜¤ëŠ˜ ì €ë… ë­ ë¨¹ì„ê¹Œ?',\n",
                "        'meta': {'sender_type': 'registered', 'in_contacts': True, 'is_first_contact': False},\n",
                "        'expected': 'ì•ˆì „'\n",
                "    },\n",
                "    {\n",
                "        'text': 'íšŒì˜ ì‹œê°„ì´ 3ì‹œë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.',\n",
                "        'meta': {'sender_type': 'registered', 'in_contacts': True, 'is_first_contact': False},\n",
                "        'expected': 'ì•ˆì „'\n",
                "    },\n",
                "    {\n",
                "        'text': 'ì—„ë§ˆ ë‚˜ ê¸‰íˆ ëˆ ì¢€ ë³´ë‚´ì¤˜',\n",
                "        'meta': {'sender_type': 'registered', 'in_contacts': True, 'is_first_contact': False},\n",
                "        'expected': 'ì•ˆì „/ì£¼ì˜'\n",
                "    },\n",
                "]\n",
                "\n",
                "print('=' * 70)\n",
                "print('ğŸ›¡ï¸ ScamShield Family v8.1 Demo (100ì  ë§Œì )')\n",
                "print('=' * 70)\n",
                "\n",
                "results = []\n",
                "for i, case in enumerate(test_cases, 1):\n",
                "    result = detector.analyze(case['text'], case['meta'])\n",
                "    results.append(result)\n",
                "    \n",
                "    print(f\"\\n[{i}] {result['grade']} ({result['final_score']}/{result['max_score']}ì )\")\n",
                "    print(f\"    ë©”ì‹œì§€: {result['text']}\")\n",
                "    print(f\"    AI: {result['ai_score']}/{result['ai_max']}ì  | ë©”íƒ€: {result['meta_score']}/{result['meta_max']}ì \")\n",
                "    \n",
                "    alert = guardian_system.send_alert(result)\n",
                "    if alert:\n",
                "        print(f\"    â¡ï¸ ë³´í˜¸ì ì•Œë¦¼: {alert['recipients']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ê²€ì¦ ê²°ê³¼ ì‹œê°í™”\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. ìœ„í—˜ ë“±ê¸‰ë³„ ë¶„í¬\n",
                "grade_counts = {'ì•ˆì „': 0, 'ì£¼ì˜': 0, 'ìœ„í—˜': 0, 'ê¸´ê¸‰': 0}\n",
                "for r in results:\n",
                "    if r['final_score'] >= 75:\n",
                "        grade_counts['ê¸´ê¸‰'] += 1\n",
                "    elif r['final_score'] >= 50:\n",
                "        grade_counts['ìœ„í—˜'] += 1\n",
                "    elif r['final_score'] >= 25:\n",
                "        grade_counts['ì£¼ì˜'] += 1\n",
                "    else:\n",
                "        grade_counts['ì•ˆì „'] += 1\n",
                "\n",
                "colors = ['#2ecc71', '#f1c40f', '#e67e22', '#e74c3c']\n",
                "axes[0, 0].pie(list(grade_counts.values()), labels=list(grade_counts.keys()),\n",
                "               colors=colors, autopct='%1.0f%%', startangle=90)\n",
                "axes[0, 0].set_title('ìœ„í—˜ ë“±ê¸‰ë³„ ë¶„í¬', fontsize=14)\n",
                "\n",
                "# 2. AI vs ë©”íƒ€ ìŠ¤ì½”ì–´\n",
                "ai_scores = [r['ai_score'] for r in results]\n",
                "meta_scores = [r['meta_score'] for r in results]\n",
                "final_scores = [r['final_score'] for r in results]\n",
                "\n",
                "scatter = axes[0, 1].scatter(ai_scores, meta_scores, c=final_scores, cmap='RdYlGn_r', s=200, edgecolors='black', vmin=0, vmax=100)\n",
                "axes[0, 1].set_xlabel('AI ëª¨ë¸ ì ìˆ˜ (0-30)', fontsize=12)\n",
                "axes[0, 1].set_ylabel('ë©”íƒ€ ìŠ¤ì½”ì–´ (0-70)', fontsize=12)\n",
                "axes[0, 1].set_title('AI vs ë©”íƒ€ ìŠ¤ì½”ì–´', fontsize=14)\n",
                "axes[0, 1].set_xlim(-2, 35)\n",
                "axes[0, 1].set_ylim(-5, 75)\n",
                "plt.colorbar(scatter, ax=axes[0, 1], label='ìµœì¢… ì ìˆ˜ (100ì  ë§Œì )')\n",
                "\n",
                "# 3. í”¼ì²˜ ê°€ì¤‘ì¹˜ (100ì  ê¸°ì¤€)\n",
                "feature_importance = {\n",
                "    'ë°œì‹ ììœ í˜•': 18, 'ì—°ë½ì²˜ë“±ë¡': 15, 'URLí¬í•¨': 12,\n",
                "    'ì²«ì—°ë½': 10, 'ê¸ˆìœµí‚¤ì›Œë“œ': 10, 'ì „í™”ë²ˆí˜¸': 7, 'ê¸´ê¸‰í‚¤ì›Œë“œ': 6\n",
                "}\n",
                "bars = axes[1, 0].barh(list(feature_importance.keys()), list(feature_importance.values()), color='#3498db')\n",
                "axes[1, 0].set_xlabel('ê°€ì¤‘ì¹˜ (ì ìˆ˜)', fontsize=12)\n",
                "axes[1, 0].set_title('ë©”íƒ€ í”¼ì²˜ ê°€ì¤‘ì¹˜ (70ì  ë§Œì  ì¤‘)', fontsize=14)\n",
                "for bar, val in zip(bars, feature_importance.values()):\n",
                "    axes[1, 0].text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2, f'{val}ì ', va='center')\n",
                "\n",
                "# 4. í…ŒìŠ¤íŠ¸ ê²°ê³¼\n",
                "test_labels = [f'í…ŒìŠ¤íŠ¸ {i+1}' for i in range(len(results))]\n",
                "bar_colors = ['#e74c3c' if r['final_score'] >= 75 else '#e67e22' if r['final_score'] >= 50 else '#f1c40f' if r['final_score'] >= 25 else '#2ecc71' for r in results]\n",
                "\n",
                "axes[1, 1].bar(test_labels, final_scores, color=bar_colors, edgecolor='black')\n",
                "axes[1, 1].axhline(y=75, color='#e74c3c', linestyle='--', label='ê¸´ê¸‰ (75)')\n",
                "axes[1, 1].axhline(y=50, color='#e67e22', linestyle='--', label='ìœ„í—˜ (50)')\n",
                "axes[1, 1].axhline(y=25, color='#f1c40f', linestyle='--', label='ì£¼ì˜ (25)')\n",
                "axes[1, 1].set_ylabel('ìµœì¢… ì ìˆ˜ (100ì  ë§Œì )', fontsize=12)\n",
                "axes[1, 1].set_title('í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ì ìˆ˜', fontsize=14)\n",
                "axes[1, 1].set_ylim(0, 105)\n",
                "axes[1, 1].legend(loc='upper right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('evaluation_v8_1.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\nâœ… ì‹œê°í™” ì €ì¥: evaluation_v8_1.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Summary\n",
                "\n",
                "### v8.1 ì ìˆ˜ ì²´ê³„ (100ì  ë§Œì )\n",
                "\n",
                "| êµ¬ë¶„ | ì ìˆ˜ | ë¹„ì¤‘ |\n",
                "|------|------|------|\n",
                "| AI ëª¨ë¸ | 0-30ì  | 30% |\n",
                "| ë©”íƒ€ ìŠ¤ì½”ì–´ | 0-70ì  | 70% |\n",
                "| **í•©ê³„** | **100ì ** | **100%** |\n",
                "\n",
                "### ë©”íƒ€ ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ (í†µê³„ ê¸°ë°˜)\n",
                "\n",
                "| í•­ëª© | ì ìˆ˜ | ê·¼ê±° |\n",
                "|------|------|------|\n",
                "| ëª¨ë¥´ëŠ” ë²ˆí˜¸ | 18ì  | í”¼ì‹± 80% ë¯¸ë“±ë¡ ë°œì‹  |\n",
                "| ì—°ë½ì²˜ ë¯¸ë“±ë¡ | 15ì  | ë¯¸ë“±ë¡ ì‹œ í”¼í•´ 4.2ë°° |\n",
                "| URL í¬í•¨ | 12ì  | ìŠ¤íŒ¸ 67% URL í¬í•¨ |\n",
                "| ì²« ì—°ë½ | 10ì  | ì²« ì—°ë½ ì‚¬ê¸° 78% |\n",
                "| ê¸ˆìœµ í‚¤ì›Œë“œ | 10ì  | í”¼ì‹± 92% ê¸ˆìœµí‚¤ì›Œë“œ |\n",
                "| ì „í™”ë²ˆí˜¸ í¬í•¨ | 7ì  | ì½œë°± ìœ ë„ 43% |\n",
                "| ê¸´ê¸‰ í‚¤ì›Œë“œ | 6ì  | ê¸´ê¸‰ì„± í˜¸ì†Œ |\n",
                "\n",
                "### ìœ„í—˜ ë“±ê¸‰\n",
                "- ğŸ”´ ê¸´ê¸‰: 75ì  ì´ìƒ\n",
                "- ğŸŸ  ìœ„í—˜: 50-74ì \n",
                "- ğŸŸ¡ ì£¼ì˜: 25-49ì \n",
                "- ğŸŸ¢ ì•ˆì „: 0-24ì \n",
                "\n",
                "### ì°¸ê³  ìë£Œ\n",
                "- ê¸ˆìœµê°ë…ì› ë³´ì´ìŠ¤í”¼ì‹± í”¼í•´í˜„í™© (2023)\n",
                "- KISA ìŠ¤íŒ¸ ë™í–¥ ë¶„ì„ ë³´ê³ ì„œ (2023)\n",
                "- Kim et al. (2024) \"Detection of Korean Phishing Messages\"\n",
                "\n",
                "---\n",
                "*ScamShield Family v8.1 - 100ì  ë§Œì  í†µí•© ìŠ¤ì½”ì–´ë§*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
